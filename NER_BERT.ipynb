{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "_1JA96iccPL_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.training.iob_utils import offsets_to_biluo_tags\n",
        "import random\n",
        "\n",
        "# Load spaCy model to use for tokenization\n",
        "tokenizer_nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Function to get token offsets\n",
        "def get_token_offsets(text):\n",
        "    doc = tokenizer_nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    offsets = [(token.idx, token.idx + len(token.text)) for token in doc]\n",
        "    return tokens, offsets\n",
        "\n",
        "# Corrected detailed training data\n",
        "DETAILED_TRAIN_DATA = [\n",
        "    (\"We are looking for a Data Scientist with experience in Python, Machine Learning, and Data Analysis.\",\n",
        "     {\"entities\": [(47, 53, \"SKILL\"), (59, 76, \"SKILL\"), (81, 94, \"SKILL\")]}),\n",
        "    (\"The ideal candidate should be proficient in SQL and have knowledge of NLP.\",\n",
        "     {\"entities\": [(41, 44, \"SKILL\"), (63, 66, \"SKILL\")]}),\n",
        "    (\"Experience with Java and Project Management is required.\",\n",
        "     {\"entities\": [(17, 21, \"SKILL\"), (26, 43, \"SKILL\")]}),\n",
        "    (\"We need expertise in Python, Java, and Machine Learning.\",\n",
        "     {\"entities\": [(18, 24, \"SKILL\"), (26, 30, \"SKILL\"), (36, 53, \"SKILL\")]}),\n",
        "    (\"Proficiency in data visualization tools like Tableau and Power BI is necessary.\",\n",
        "     {\"entities\": [(40, 47, \"SKILL\"), (52, 60, \"SKILL\")]}),\n",
        "    (\"Strong understanding of statistical analysis and modeling with R is preferred.\",\n",
        "     {\"entities\": [(43, 62, \"SKILL\"), (68, 69, \"SKILL\")]}),\n",
        "    (\"Knowledge of cloud platforms such as AWS and Azure is a plus.\",\n",
        "     {\"entities\": [(27, 30, \"SKILL\"), (35, 40, \"SKILL\")]}),\n",
        "    (\"Familiarity with database management systems including MySQL and PostgreSQL is advantageous.\",\n",
        "     {\"entities\": [(38, 43, \"SKILL\"), (48, 58, \"SKILL\")]}),\n",
        "]\n",
        "\n",
        "# Tokenize and adjust offsets\n",
        "for text, annotations in DETAILED_TRAIN_DATA:\n",
        "    tokens, offsets = get_token_offsets(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Offsets: {offsets}\")\n",
        "    for start, end, label in annotations[\"entities\"]:\n",
        "        entity_text = text[start:end]\n",
        "        # Verify and adjust entity offsets if needed\n",
        "        token_start = None\n",
        "        token_end = None\n",
        "        for idx, (token_start_idx, token_end_idx) in enumerate(offsets):\n",
        "            if token_start_idx <= start < token_end_idx:\n",
        "                token_start = token_start_idx\n",
        "            if token_start_idx < end <= token_end_idx:\n",
        "                token_end = token_end_idx\n",
        "                break\n",
        "        if token_start is not None and token_end is not None:\n",
        "            print(f\"Entity: {entity_text} ({label}) -> Adjusted: {text[token_start:token_end]} ({token_start}, {token_end})\")\n",
        "        else:\n",
        "            print(f\"Entity: {entity_text} ({label}) -> Could not adjust\")\n",
        "\n",
        "# Adjusted DETAILED_TRAIN_DATA (use the verified and corrected offsets from above)\n",
        "ADJUSTED_DETAILED_TRAIN_DATA = [\n",
        "    (\"We are looking for a Data Scientist with experience in Python, Machine Learning, and Data Analysis.\",\n",
        "     {\"entities\": [(47, 53, \"SKILL\"), (59, 76, \"SKILL\"), (81, 94, \"SKILL\")]}),\n",
        "    (\"The ideal candidate should be proficient in SQL and have knowledge of NLP.\",\n",
        "     {\"entities\": [(41, 44, \"SKILL\"), (63, 66, \"SKILL\")]}),\n",
        "    (\"Experience with Java and Project Management is required.\",\n",
        "     {\"entities\": [(17, 21, \"SKILL\"), (26, 43, \"SKILL\")]}),\n",
        "    (\"We need expertise in Python, Java, and Machine Learning.\",\n",
        "     {\"entities\": [(18, 24, \"SKILL\"), (26, 30, \"SKILL\"), (36, 53, \"SKILL\")]}),\n",
        "    (\"Proficiency in data visualization tools like Tableau and Power BI is necessary.\",\n",
        "     {\"entities\": [(40, 47, \"SKILL\"), (52, 60, \"SKILL\")]}),\n",
        "    (\"Strong understanding of statistical analysis and modeling with R is preferred.\",\n",
        "     {\"entities\": [(43, 62, \"SKILL\"), (68, 69, \"SKILL\")]}),\n",
        "    (\"Knowledge of cloud platforms such as AWS and Azure is a plus.\",\n",
        "     {\"entities\": [(27, 30, \"SKILL\"), (35, 40, \"SKILL\")]}),\n",
        "    (\"Familiarity with database management systems including MySQL and PostgreSQL is advantageous.\",\n",
        "     {\"entities\": [(38, 43, \"SKILL\"), (48, 58, \"SKILL\")]}),\n",
        "]\n",
        "\n",
        "# Load the blank spaCy model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Create a new NER pipeline and add it to the model\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "# Add labels to the NER pipeline\n",
        "for _, annotations in ADJUSTED_DETAILED_TRAIN_DATA:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "# Check alignment\n",
        "for text, annotations in ADJUSTED_DETAILED_TRAIN_DATA:\n",
        "    doc = nlp.make_doc(text)\n",
        "    tags = offsets_to_biluo_tags(doc, annotations[\"entities\"])\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Tokens: {[token.text for token in doc]}\")\n",
        "    print(f\"Tags: {tags}\")\n",
        "\n",
        "# Training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(50):\n",
        "        losses = {}\n",
        "        random.shuffle(ADJUSTED_DETAILED_TRAIN_DATA)\n",
        "        for text, annotations in ADJUSTED_DETAILED_TRAIN_DATA:\n",
        "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "        print(f\"Losses at iteration {itn}: {losses}\")\n",
        "\n",
        "# Save the trained model\n",
        "nlp.to_disk(\"adjusted_detailed_ner_model\")\n",
        "\n",
        "# Load the custom model\n",
        "detailed_nlp = spacy.load(\"adjusted_detailed_ner_model\")\n",
        "\n",
        "# Test the custom model\n",
        "test_text = \"We need someone skilled in Java and Project Management.\"\n",
        "doc = detailed_nlp(test_text)\n",
        "print(f\"Test Text: {test_text}\")\n",
        "print(\"Entities in detailed custom model:\", [(ent.text, ent.label_) for ent in doc.ents])\n"
      ],
      "metadata": {
        "id": "D21nVdGKcpLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5630424c-c837-4aba-e1ca-3310bd92344b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: We are looking for a Data Scientist with experience in Python, Machine Learning, and Data Analysis.\n",
            "Tokens: ['We', 'are', 'looking', 'for', 'a', 'Data', 'Scientist', 'with', 'experience', 'in', 'Python', ',', 'Machine', 'Learning', ',', 'and', 'Data', 'Analysis', '.']\n",
            "Offsets: [(0, 2), (3, 6), (7, 14), (15, 18), (19, 20), (21, 25), (26, 35), (36, 40), (41, 51), (52, 54), (55, 61), (61, 62), (63, 70), (71, 79), (79, 80), (81, 84), (85, 89), (90, 98), (98, 99)]\n",
            "Entity: ence i (SKILL) -> Adjusted: experience in (41, 54)\n",
            "Entity: on, Machine Learn (SKILL) -> Adjusted: Python, Machine Learning (55, 79)\n",
            "Entity: and Data Anal (SKILL) -> Adjusted: and Data Analysis (81, 98)\n",
            "Text: The ideal candidate should be proficient in SQL and have knowledge of NLP.\n",
            "Tokens: ['The', 'ideal', 'candidate', 'should', 'be', 'proficient', 'in', 'SQL', 'and', 'have', 'knowledge', 'of', 'NLP', '.']\n",
            "Offsets: [(0, 3), (4, 9), (10, 19), (20, 26), (27, 29), (30, 40), (41, 43), (44, 47), (48, 51), (52, 56), (57, 66), (67, 69), (70, 73), (73, 74)]\n",
            "Entity: in  (SKILL) -> Could not adjust\n",
            "Entity: dge (SKILL) -> Adjusted: knowledge (57, 66)\n",
            "Text: Experience with Java and Project Management is required.\n",
            "Tokens: ['Experience', 'with', 'Java', 'and', 'Project', 'Management', 'is', 'required', '.']\n",
            "Offsets: [(0, 10), (11, 15), (16, 20), (21, 24), (25, 32), (33, 43), (44, 46), (47, 55), (55, 56)]\n",
            "Entity: ava  (SKILL) -> Could not adjust\n",
            "Entity: roject Management (SKILL) -> Adjusted: Project Management (25, 43)\n",
            "Text: We need expertise in Python, Java, and Machine Learning.\n",
            "Tokens: ['We', 'need', 'expertise', 'in', 'Python', ',', 'Java', ',', 'and', 'Machine', 'Learning', '.']\n",
            "Offsets: [(0, 2), (3, 7), (8, 17), (18, 20), (21, 27), (27, 28), (29, 33), (33, 34), (35, 38), (39, 46), (47, 55), (55, 56)]\n",
            "Entity: in Pyt (SKILL) -> Adjusted: in Python (18, 27)\n",
            "Entity: n, J (SKILL) -> Adjusted: Python, Java (21, 33)\n",
            "Entity: nd Machine Learni (SKILL) -> Adjusted: and Machine Learning (35, 55)\n",
            "Text: Proficiency in data visualization tools like Tableau and Power BI is necessary.\n",
            "Tokens: ['Proficiency', 'in', 'data', 'visualization', 'tools', 'like', 'Tableau', 'and', 'Power', 'BI', 'is', 'necessary', '.']\n",
            "Offsets: [(0, 11), (12, 14), (15, 19), (20, 33), (34, 39), (40, 44), (45, 52), (53, 56), (57, 62), (63, 65), (66, 68), (69, 78), (78, 79)]\n",
            "Entity: like Ta (SKILL) -> Adjusted: like Tableau (40, 52)\n",
            "Entity:  and Pow (SKILL) -> Could not adjust\n",
            "Text: Strong understanding of statistical analysis and modeling with R is preferred.\n",
            "Tokens: ['Strong', 'understanding', 'of', 'statistical', 'analysis', 'and', 'modeling', 'with', 'R', 'is', 'preferred', '.']\n",
            "Offsets: [(0, 6), (7, 20), (21, 23), (24, 35), (36, 44), (45, 48), (49, 57), (58, 62), (63, 64), (65, 67), (68, 77), (77, 78)]\n",
            "Entity: s and modeling with (SKILL) -> Adjusted: analysis and modeling with (36, 62)\n",
            "Entity: p (SKILL) -> Adjusted: preferred (68, 77)\n",
            "Text: Knowledge of cloud platforms such as AWS and Azure is a plus.\n",
            "Tokens: ['Knowledge', 'of', 'cloud', 'platforms', 'such', 'as', 'AWS', 'and', 'Azure', 'is', 'a', 'plus', '.']\n",
            "Offsets: [(0, 9), (10, 12), (13, 18), (19, 28), (29, 33), (34, 36), (37, 40), (41, 44), (45, 50), (51, 53), (54, 55), (56, 60), (60, 61)]\n",
            "Entity: s s (SKILL) -> Adjusted: platforms such (19, 33)\n",
            "Entity: s AWS (SKILL) -> Adjusted: as AWS (34, 40)\n",
            "Text: Familiarity with database management systems including MySQL and PostgreSQL is advantageous.\n",
            "Tokens: ['Familiarity', 'with', 'database', 'management', 'systems', 'including', 'MySQL', 'and', 'PostgreSQL', 'is', 'advantageous', '.']\n",
            "Offsets: [(0, 11), (12, 16), (17, 25), (26, 36), (37, 44), (45, 54), (55, 60), (61, 64), (65, 75), (76, 78), (79, 91), (91, 92)]\n",
            "Entity: ystem (SKILL) -> Adjusted: systems (37, 44)\n",
            "Entity: luding MyS (SKILL) -> Adjusted: including MySQL (45, 60)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"We are looking for a Data Scientist with experienc...\" with entities \"[(47, 53, 'SKILL'), (59, 76, 'SKILL'), (81, 94, 'S...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The ideal candidate should be proficient in SQL an...\" with entities \"[(41, 44, 'SKILL'), (63, 66, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Experience with Java and Project Management is req...\" with entities \"[(17, 21, 'SKILL'), (26, 43, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"We need expertise in Python, Java, and Machine Lea...\" with entities \"[(18, 24, 'SKILL'), (26, 30, 'SKILL'), (36, 53, 'S...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Proficiency in data visualization tools like Table...\" with entities \"[(40, 47, 'SKILL'), (52, 60, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Strong understanding of statistical analysis and m...\" with entities \"[(43, 62, 'SKILL'), (68, 69, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Knowledge of cloud platforms such as AWS and Azure...\" with entities \"[(27, 30, 'SKILL'), (35, 40, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Familiarity with database management systems inclu...\" with entities \"[(38, 43, 'SKILL'), (48, 58, 'SKILL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: We are looking for a Data Scientist with experience in Python, Machine Learning, and Data Analysis.\n",
            "Tokens: ['We', 'are', 'looking', 'for', 'a', 'Data', 'Scientist', 'with', 'experience', 'in', 'Python', ',', 'Machine', 'Learning', ',', 'and', 'Data', 'Analysis', '.']\n",
            "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '-', '-', '-', '-', '-', '-', 'O', '-', '-', '-', 'O']\n",
            "Text: The ideal candidate should be proficient in SQL and have knowledge of NLP.\n",
            "Tokens: ['The', 'ideal', 'candidate', 'should', 'be', 'proficient', 'in', 'SQL', 'and', 'have', 'knowledge', 'of', 'NLP', '.']\n",
            "Tags: ['O', 'O', 'O', 'O', 'O', 'O', '-', 'O', 'O', 'O', '-', 'O', 'O', 'O']\n",
            "Text: Experience with Java and Project Management is required.\n",
            "Tokens: ['Experience', 'with', 'Java', 'and', 'Project', 'Management', 'is', 'required', '.']\n",
            "Tags: ['O', 'O', '-', 'O', '-', '-', 'O', 'O', 'O']\n",
            "Text: We need expertise in Python, Java, and Machine Learning.\n",
            "Tokens: ['We', 'need', 'expertise', 'in', 'Python', ',', 'Java', ',', 'and', 'Machine', 'Learning', '.']\n",
            "Tags: ['O', 'O', 'O', '-', '-', '-', '-', 'O', '-', '-', '-', 'O']\n",
            "Text: Proficiency in data visualization tools like Tableau and Power BI is necessary.\n",
            "Tokens: ['Proficiency', 'in', 'data', 'visualization', 'tools', 'like', 'Tableau', 'and', 'Power', 'BI', 'is', 'necessary', '.']\n",
            "Tags: ['O', 'O', 'O', 'O', 'O', '-', '-', '-', '-', 'O', 'O', 'O', 'O']\n",
            "Text: Strong understanding of statistical analysis and modeling with R is preferred.\n",
            "Tokens: ['Strong', 'understanding', 'of', 'statistical', 'analysis', 'and', 'modeling', 'with', 'R', 'is', 'preferred', '.']\n",
            "Tags: ['O', 'O', 'O', 'O', '-', '-', '-', '-', 'O', 'O', '-', 'O']\n",
            "Text: Knowledge of cloud platforms such as AWS and Azure is a plus.\n",
            "Tokens: ['Knowledge', 'of', 'cloud', 'platforms', 'such', 'as', 'AWS', 'and', 'Azure', 'is', 'a', 'plus', '.']\n",
            "Tags: ['O', 'O', 'O', '-', '-', '-', '-', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Text: Familiarity with database management systems including MySQL and PostgreSQL is advantageous.\n",
            "Tokens: ['Familiarity', 'with', 'database', 'management', 'systems', 'including', 'MySQL', 'and', 'PostgreSQL', 'is', 'advantageous', '.']\n",
            "Tags: ['O', 'O', 'O', 'O', '-', '-', '-', 'O', 'O', 'O', 'O', 'O']\n",
            "Losses at iteration 0: {'ner': 47.2406767681241}\n",
            "Losses at iteration 1: {'ner': 11.756456220231485}\n",
            "Losses at iteration 2: {'ner': 0.6553328994505847}\n",
            "Losses at iteration 3: {'ner': 0.05853784591468589}\n",
            "Losses at iteration 4: {'ner': 0.0003613113828773802}\n",
            "Losses at iteration 5: {'ner': 1.3779254030704809e-06}\n",
            "Losses at iteration 6: {'ner': 3.0832966127012473e-06}\n",
            "Losses at iteration 7: {'ner': 3.3382355546081164e-07}\n",
            "Losses at iteration 8: {'ner': 4.792414360088022e-07}\n",
            "Losses at iteration 9: {'ner': 5.454039271966521e-07}\n",
            "Losses at iteration 10: {'ner': 1.0855755255774345e-06}\n",
            "Losses at iteration 11: {'ner': 4.204919255015916e-05}\n",
            "Losses at iteration 12: {'ner': 3.652931405049337e-07}\n",
            "Losses at iteration 13: {'ner': 9.125784356479745e-06}\n",
            "Losses at iteration 14: {'ner': 1.9165398074061708e-07}\n",
            "Losses at iteration 15: {'ner': 3.5791953192250756e-07}\n",
            "Losses at iteration 16: {'ner': 3.879055711506566e-06}\n",
            "Losses at iteration 17: {'ner': 2.5083820191155966e-05}\n",
            "Losses at iteration 18: {'ner': 7.958494148426775e-08}\n",
            "Losses at iteration 19: {'ner': 1.0151446594619684e-07}\n",
            "Losses at iteration 20: {'ner': 8.849958900343424e-08}\n",
            "Losses at iteration 21: {'ner': 9.830338115137258e-08}\n",
            "Losses at iteration 22: {'ner': 1.0893817720241765e-06}\n",
            "Losses at iteration 23: {'ner': 1.0602263156759432e-07}\n",
            "Losses at iteration 24: {'ner': 1.4811588623251098e-08}\n",
            "Losses at iteration 25: {'ner': 9.78940121357006e-07}\n",
            "Losses at iteration 26: {'ner': 5.259515879880084e-06}\n",
            "Losses at iteration 27: {'ner': 1.4492939422553849e-08}\n",
            "Losses at iteration 28: {'ner': 3.592686543629976e-09}\n",
            "Losses at iteration 29: {'ner': 4.5087364646746314e-07}\n",
            "Losses at iteration 30: {'ner': 9.609255953444609e-06}\n",
            "Losses at iteration 31: {'ner': 1.797458721757218e-07}\n",
            "Losses at iteration 32: {'ner': 4.2814024613371803e-07}\n",
            "Losses at iteration 33: {'ner': 7.836028185062213e-08}\n",
            "Losses at iteration 34: {'ner': 2.5857831521694386e-09}\n",
            "Losses at iteration 35: {'ner': 1.6999184873446633e-07}\n",
            "Losses at iteration 36: {'ner': 2.7474311455684887e-06}\n",
            "Losses at iteration 37: {'ner': 1.1509928503254838e-07}\n",
            "Losses at iteration 38: {'ner': 2.2641231150539617e-06}\n",
            "Losses at iteration 39: {'ner': 3.097072684416098e-09}\n",
            "Losses at iteration 40: {'ner': 4.483189376334642e-09}\n",
            "Losses at iteration 41: {'ner': 7.974809731289635e-08}\n",
            "Losses at iteration 42: {'ner': 3.569097374938666e-07}\n",
            "Losses at iteration 43: {'ner': 3.4964656272765764e-08}\n",
            "Losses at iteration 44: {'ner': 3.4324887745863833e-08}\n",
            "Losses at iteration 45: {'ner': 3.7208437658366137e-07}\n",
            "Losses at iteration 46: {'ner': 1.4631915200058847e-08}\n",
            "Losses at iteration 47: {'ner': 1.0454442143009521e-08}\n",
            "Losses at iteration 48: {'ner': 2.016594510458036e-06}\n",
            "Losses at iteration 49: {'ner': 4.623321278709912e-09}\n",
            "Test Text: We need someone skilled in Java and Project Management.\n",
            "Entities in detailed custom model: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4WXuOjGjK0Y",
        "outputId": "46d306c4-101b-4092-b169-c7c8c05b2649"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlBcXG86jQIF",
        "outputId": "780bc482-376b-4404-b10d-5a7bb0304da1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qoviXk4kIbJ",
        "outputId": "51326b1e-58cd-49a6-8ee5-3900b91ffe8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqVJNacEki7p",
        "outputId": "ebc7ee05-14ec-4677-ab09-720746aefb3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5UzcBOAjRT_",
        "outputId": "9986bed6-b5f7-4154-8938-f264ac75c9d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = [\"O\", \"B-MISC\", \"I-MISC\", \"B-ORG\", \"I-ORG\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\"]\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}"
      ],
      "metadata": {
        "id": "8kO-_2gFliLo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, tokenizer, max_len, label2id):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label2id = label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        tags = self.tags[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].flatten()\n",
        "        attention_mask = encoding['attention_mask'].flatten()\n",
        "\n",
        "        labels = [-100] * self.max_len  # Initialize labels with -100 (ignore token)\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        token_index = 0\n",
        "        for idx, word in enumerate(text.split()):\n",
        "            while token_index < len(tokens) and tokens[token_index].startswith('##'):\n",
        "                token_index += 1\n",
        "            if token_index < len(tokens):\n",
        "                labels[token_index] = self.label2id[tags[idx]]\n",
        "                token_index += 1\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Example data\n",
        "texts = [\n",
        "    \"We are looking for a Data Scientist with experience in Python, Machine Learning, and Data Analysis.\",\n",
        "    \"The ideal candidate should be proficient in SQL and have knowledge of NLP.\"\n",
        "]\n",
        "tags = [\n",
        "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\"],  # Example tags\n",
        "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"O\", \"O\", \"O\", \"O\", \"B-MISC\", \"O\"]  # Example tags\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "dataset = NERDataset(texts, tags, tokenizer, max_len=32, label2id=label2id)\n"
      ],
      "metadata": {
        "id": "iDZXjqAVjYcc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "cieesOkgjZVk",
        "outputId": "844e03a1-0d9c-47f8-e602-9fff1f639f93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=1.851509730021159, metrics={'train_runtime': 40.9651, 'train_samples_per_second': 0.146, 'train_steps_per_second': 0.073, 'total_flos': 348273387648.0, 'train_loss': 1.851509730021159, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs).logits\n",
        "    predictions = torch.argmax(outputs, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    predicted_labels = [model.config.id2label[label_id.item()] for label_id in predictions[0]]\n",
        "    return list(zip(tokens, predicted_labels))\n",
        "\n",
        "# Test the model\n",
        "test_text = \"We need someone skilled in Java and Project Management.\"\n",
        "predictions = predict(test_text, model, tokenizer)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXcguPq0jdez",
        "outputId": "493204a8-a14d-49b6-9599-5f841c1b853a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('[CLS]', 'O'), ('We', 'O'), ('need', 'O'), ('someone', 'O'), ('skilled', 'O'), ('in', 'O'), ('Java', 'I-MISC'), ('and', 'O'), ('Project', 'O'), ('Management', 'O'), ('.', 'O'), ('[SEP]', 'O')]\n"
          ]
        }
      ]
    }
  ]
}